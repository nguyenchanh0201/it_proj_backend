import os
import torch
from threading import Thread
from celery import Celery
from transformers import (
    AutoModelForCausalLM, 
    AutoModelForImageTextToText, 
    AutoProcessor, 
    AutoTokenizer, 
    TextIteratorStreamer
)

# --- C·∫§U H√åNH H·ªÜ TH·ªêNG ---
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")
MODEL_TYPE = os.getenv("MODEL_TYPE", "qwen").lower() # M·∫∑c ƒë·ªãnh l√† qwen
HF_TOKEN = os.getenv("HF_TOKEN", "")

# --- KH·ªûI T·∫†O CELERY ---
celery_app = Celery('worker_app', broker=REDIS_URL, backend=REDIS_URL)
celery_app.conf.update(
    timezone='Asia/Ho_Chi_Minh',
    broker_connection_retry_on_startup=True,
    task_track_started=True
)

# --- ƒê·ªäNH NGHƒ®A DANH S√ÅCH MODEL ---
MODELS_CONFIG = {
    "qwen": {
        "repo": "Qwen/Qwen3-VL-4B-Instruct",
        "class": AutoModelForImageTextToText,
        "loader": "processor"
    },
    "gemma": {
        "repo": "google/gemma-3-4b-it",
        "class": AutoModelForCausalLM,
        "loader": "processor"
    },
    "phi": {
        "repo": "microsoft/Phi-3.5-vision-instruct",
        "class": AutoModelForCausalLM,
        "loader": "processor"
    },
    "llama": {
        "repo": "meta-llama/Llama-3.2-3B-Instruct",
        "class": AutoModelForCausalLM,
        "loader": "tokenizer"
    }
}

# --- LOGIC LOAD MODEL ƒê·ªòNG ---
print(f"üöÄ Kh·ªüi t·∫°o Worker cho lo·∫°i Model: {MODEL_TYPE.upper()}")

model, processor, tokenizer = None, None, None
config = MODELS_CONFIG.get(MODEL_TYPE)

if config:
    try:
        # Load Model
        model = config["class"].from_pretrained(
            config["repo"],
            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else "auto",
            device_map="auto",
            trust_remote_code=True,
            token=HF_TOKEN
        )
        
        # Load Processor ho·∫∑c Tokenizer
        if config["loader"] == "processor":
            processor = AutoProcessor.from_pretrained(config["repo"], trust_remote_code=True, token=HF_TOKEN)
            tokenizer = processor.tokenizer if hasattr(processor, 'tokenizer') else processor
        else:
            tokenizer = AutoTokenizer.from_pretrained(config["repo"], token=HF_TOKEN)
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            processor = tokenizer # ƒê·ªÉ d√πng chung bi·∫øn ·ªü d∆∞·ªõi

        print(f"‚úÖ {MODEL_TYPE.upper()} ƒë√£ s·∫µn s√†ng tr√™n {model.device}!")
    except Exception as e:
        print(f"‚ùå L·ªói load model {MODEL_TYPE}: {e}")
else:
    print(f"‚ö†Ô∏è MODEL_TYPE '{MODEL_TYPE}' kh√¥ng h·ª£p l·ªá!")

# --- CELERY TASK CHUNG ---
@celery_app.task(bind=True, name="generate_mermaid_task")
def generate_mermaid_task(self, scenario_description: str) -> dict:
    if model is None:
        return {"error": f"Model {MODEL_TYPE} ch∆∞a ƒë∆∞·ª£c n·∫°p th√†nh c√¥ng."}

    self.update_state(state='PROGRESS', meta={'percent': 5, 'message': 'ƒêang chu·∫©n b·ªã prompt...'})

    # T√πy ch·ªânh System Prompt theo t·ª´ng lo·∫°i model n·∫øu c·∫ßn
    system_prompt = "You are an expert Software Architect. Convert the user scenario into a Mermaid.js `sequenceDiagram`. Return ONLY the code block."
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": scenario_description}
    ]

    # Ti·ªÅn x·ª≠ l√Ω (X·ª≠ l√Ω kh√°c bi·ªát gi·ªØa Processor v√† Tokenizer)
    if MODEL_TYPE == "llama":
        inputs_data = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt")
        inputs = {"input_ids": inputs_data.to(model.device)}
    else:
        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = processor(text=[text], padding=True, return_tensors="pt").to(model.device)

    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
    generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=1024, temperature=0.1)

    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()

    generated_text = ""
    token_count = 0

    for new_text in streamer:
        generated_text += new_text
        token_count += 1
        if token_count % 5 == 0:
            self.update_state(state='PROGRESS', meta={
                'percent': min(10 + (token_count // 3), 95),
                'message': f'{MODEL_TYPE} ƒëang vi·∫øt... ({token_count} tokens)',
                'partial_result': generated_text
            })

    thread.join()
    return {
        "status": "completed",
        "model": config["repo"],
        "mermaid_code": generated_text.strip()
    }
